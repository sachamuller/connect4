{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from cart_pole import DQN, ReplayMemory, Transition, train\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       2., 1., 0., 1., 2., 0., 0., 2.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def state_observation_to_DQN_input(state_observation, agent_selection, current_player, my_value=1, ennemy_value=2):\n",
    "    if current_player == agent_selection : \n",
    "        my_pieces = 0\n",
    "        adverse_pieces = 1\n",
    "    else : \n",
    "        my_pieces = 1\n",
    "        adverse_pieces = 0\n",
    "    result = np.zeros(state_observation.shape[:2])\n",
    "    result[np.where(state_observation[:, :, my_pieces] == 1)] = my_value\n",
    "    result[np.where(state_observation[:, :, adverse_pieces] == 1)] = ennemy_value\n",
    "    return result.flatten()\n",
    "\n",
    "env.reset()\n",
    "env.step(0) # 0 \n",
    "env.step(6) # 1 \n",
    "env.step(0) # 0\n",
    "env.step(6) # 1\n",
    "env.step(2) # 0\n",
    "env.step(3)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "print(env.agent_selection)\n",
    "state_observation_to_DQN_input(state[\"observation\"], env.agent_selection, 'player_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7, 2)\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "# The main difference with the standard gym api is the way the environment is queried. The `step` method return `None`. To get the data on the environment, use the `last` method\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "# state is a dictionary with two keys: observation and action_mask\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ")  # Observation is a numpy array with three coordinates, indicating the positions of the pieces of of player 0 and 1 on the the board\n",
    "print(state[\"observation\"][:, :, 0])  # Where the pieces of player 0 are\n",
    "print(state[\"observation\"][:, :, 1])  # Where the pieces of player 1 are\n",
    "\n",
    "print(state[\"action_mask\"])  # an array showing whether the actions are legal or nots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdamW(policy_net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLR, amsgrad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m memory \u001b[39m=\u001b[39m ReplayMemory(\u001b[39m10000\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m train(\n\u001b[1;32m     31\u001b[0m     env,\n\u001b[1;32m     32\u001b[0m     BATCH_SIZE,\n\u001b[1;32m     33\u001b[0m     GAMMA,\n\u001b[1;32m     34\u001b[0m     EPS_START,\n\u001b[1;32m     35\u001b[0m     EPS_END,\n\u001b[1;32m     36\u001b[0m     EPS_DECAY,\n\u001b[1;32m     37\u001b[0m     TAU,\n\u001b[1;32m     38\u001b[0m     optimizer,\n\u001b[1;32m     39\u001b[0m     memory,\n\u001b[1;32m     40\u001b[0m     policy_net,\n\u001b[1;32m     41\u001b[0m     target_net,\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Scolaire/5-CentraleSupelec/3A/7-SM11/RL/connect4/cart_pole.py:160\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, optimizer, memory, policy_net, target_net, num_episodes)\u001b[0m\n\u001b[1;32m    156\u001b[0m steps_done \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m i_episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m    159\u001b[0m     \u001b[39m# Initialize the environment and get it's state\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     state, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    161\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "BATCH_SIZE=128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space(env.possible_agents[0]).n\n",
    "# Get the number of state observations\n",
    "env.reset()\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "n_observations = state[\"observation\"].shape[0] * state[\"observation\"].shape[1]\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    BATCH_SIZE,\n",
    "    GAMMA,\n",
    "    EPS_START,\n",
    "    EPS_END,\n",
    "    EPS_DECAY,\n",
    "    TAU,\n",
    "    optimizer,\n",
    "    memory,\n",
    "    policy_net,\n",
    "    target_net,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5971313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b94de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f5bcdf",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here are some implementations of trivial agents that you should be able to beat ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f7e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            self.rng = np.random.default_rng()\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self.name = \"Random Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        return self.random_choice_with_mask(np.arange(7), obs_mask[\"action_mask\"])\n",
    "\n",
    "    def random_choice_with_mask(self, arr, mask):\n",
    "        masked_arr = np.ma.masked_array(arr, mask=1 - mask)\n",
    "        if masked_arr.count() == 0:\n",
    "            return None\n",
    "        return self.rng.choice(masked_arr.compressed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfcac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayLeftmostLegal:\n",
    "    def __init__(self):\n",
    "        self.name = \"Left Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        for i, legal in enumerate(obs_mask[\"action_mask\"]):\n",
    "            if legal:\n",
    "                return i\n",
    "        return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4fdf7e",
   "metadata": {},
   "source": [
    "# Running a game\n",
    "\n",
    "\n",
    "The following function runs a full game between the two agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent0, agent1, display=False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    while not done:\n",
    "        for i, agent in enumerate([agent0, agent1]):\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            env.step(action)\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            done = terminated\n",
    "            if np.sum(obs[\"action_mask\"]) == 0:\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                return 0.5\n",
    "            if done:\n",
    "                if display:\n",
    "                    print(f\"Player {i}: {agent.name} won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7601a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent0 = RandomPlayer()\n",
    "# agent1 = PlayLeftmostLegal()\n",
    "\n",
    "# play_game(env, agent0, agent1, display=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173cc5af",
   "metadata": {},
   "source": [
    "# Emulating a Gym environment\n",
    "\n",
    "If we fix the opposite policy, the game from the point of view of the agent is equivalent to a Gym environment. The following class implements this simulation. Then any algorithm that would work in a gym environment with the same observations will work here. \n",
    "\n",
    "Note that we implemented the possibility to be the first or the second player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a2406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvAgainstPolicy: \n",
    "    def __init__(self, env, policy, first_player=True):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.first_player = first_player\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward, terminated, _, _ = self.env.last()\n",
    "        if terminated: \n",
    "            self.last_step = obs, reward, True, False, {}\n",
    "        else: \n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "            obs, reward, terminated, _, _ = self.env.last()\n",
    "            self.last_step = obs, -reward, terminated, False, {}\n",
    "        return self.last_step\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        if not(self.first_player): \n",
    "            obs, _, _, _, _ = self.env.last()\n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "\n",
    "        self.last_step = self.env.last()\n",
    "        return self.last_step\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb3fab1",
   "metadata": {},
   "source": [
    "# Evaluating an agent against a fixed policy: \n",
    "\n",
    "Using the environment above, we can evaluate the agent against this fixed policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_policy(env, agent, policy, n_episodes=10, first_player=True):\n",
    "    eval_env = EnvAgainstPolicy(env, policy, first_player=first_player)\n",
    "    results = []\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        eval_env.reset()\n",
    "        obs, _, _, _, _ = eval_env.last()\n",
    "        while not done:\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            eval_env.step(action)\n",
    "            obs, reward, done, _, _ = eval_env.last()\n",
    "        results.append(reward)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da1790b4",
   "metadata": {},
   "source": [
    "We can see that if both players play randomly, there is a small but significant advantage to the first player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38debdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), n_episodes=1000, first_player=False))\n",
    "# plt.show()\n",
    "# plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), n_episodes=1000, first_player=True))\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79580594",
   "metadata": {},
   "source": [
    "# Your turn \n",
    "\n",
    "Try to build a decent agent. Be creative! You can try any idea that you have: the grade is not about performance of the agent, but more about illustrating phenomena happening in Reinforcement Learning for turn-based games. It's okay to 'help' the agent in any way, as long as it follows the ideas of RL (i.e., as long as there is some learning involved).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9bc51a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/sachamuller/Documents/Scolaire/5-CentraleSupelec/3A/7-SM11/RL/connect4/utils.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49432f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "batch_size = 128\n",
    "buffer_capacity = 10_000\n",
    "update_target_every = 32\n",
    "\n",
    "epsilon_start = 0.9\n",
    "decrease_epsilon_factor = 1000\n",
    "epsilon_min = 0.05\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "DQN_agent = utils.DQN_Skeleton(\n",
    "        action_space_size=7, # number of columns\n",
    "        observation_space_size = 6*7,  # nb_rows, nb_columns\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        buffer_capacity=buffer_capacity,\n",
    "        update_target_every=update_target_every,\n",
    "        epsilon_start=epsilon_start,\n",
    "        decrease_epsilon_factor=decrease_epsilon_factor,\n",
    "        epsilon_min=epsilon_min,\n",
    "        learning_rate=learning_rate,\n",
    "        env=env,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33aa749c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, -1, 1, -1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_player = RandomPlayer()\n",
    "eval_against_policy(env, DQN_agent, random_player, n_episodes=10, first_player=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0cfc9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_player_0(env, player_0, player_1, n_episodes, eval_every=50, reward_threshold=0.9):\n",
    "    \n",
    "    agents = [player_0, player_1]\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes), desc=\"Train\"):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        length_episode = 0\n",
    "        while not done:\n",
    "            length_episode += 1\n",
    "            for agent in agents:\n",
    "                next_state, reward, terminated, truncated, info = env.last()\n",
    "                done = terminated or truncated\n",
    "                if done:\n",
    "                    break \n",
    "                else:\n",
    "                    action = agent.get_action(next_state)\n",
    "                    env.step(action)\n",
    "                    if agent == player_0 :\n",
    "                        loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "                        losses.append(loss_val)\n",
    "                        state = next_state   \n",
    "\n",
    "                    \n",
    "        if ep%eval_every == 0:\n",
    "            print(\"[Train] Evaluating the DQN Agent.\")\n",
    "            rewards = eval_against_policy(env, player_0, player_1, n_episodes=10, first_player=True)\n",
    "            all_rewards.append(np.mean(rewards))\n",
    "            print(f\"Mean reward is: {np.mean(rewards)}\")\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses, all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14108b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/100 [00:00<?, ?it/s]/Users/sachamuller/Documents/Scolaire/5-CentraleSupelec/3A/7-SM11/RL/connect4/utils.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(state_tensor).unsqueeze(0),\n",
      "/Users/sachamuller/Documents/Scolaire/5-CentraleSupelec/3A/7-SM11/RL/connect4/utils.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(next_state_tensor).unsqueeze(0),\n",
      "Train:   8%|▊         | 8/100 [00:00<00:02, 43.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Evaluating the DQN Agent.\n",
      "Mean reward is: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  53%|█████▎    | 53/100 [00:02<00:02, 21.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Evaluating the DQN Agent.\n",
      "Mean reward is: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 100/100 [00:04<00:00, 23.67it/s]\n"
     ]
    }
   ],
   "source": [
    "random_player = RandomPlayer()\n",
    "losses = train_player_0(env, DQN_agent, random_player, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

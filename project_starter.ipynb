{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5971313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd69e510",
   "metadata": {},
   "source": [
    "# Using the PettingZoo environment\n",
    "\n",
    "This notebook provides smalls chunks of code to get you started with the Connect4 project. You do not have to use this code in you final file, but you can if you wish to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc87362",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# The main difference with the standard gym api is the way the environment is queried. The `step` method return `None`. To get the data on the environment, use the `last` method\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "# state is a dictionary with two keys: observation and action_mask\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ")  # Observation is a numpy array with three coordinates, indicating the positions of the pieces of of player 0 and 1 on the the board\n",
    "print(state[\"observation\"][:, :, 0])  # Where the pieces of player 0 are\n",
    "print(state[\"observation\"][:, :, 1])  # Where the pieces of player 1 are\n",
    "\n",
    "print(state[\"action_mask\"])  # an array showing whether the actions are legal or nots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ") \n",
    "print(state[\"observation\"][:, :, 0])  \n",
    "print(state[\"observation\"][:, :, 1])  \n",
    "\n",
    "print(state[\"action_mask\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f5bcdf",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here are some implementations of trivial agents that you should be able to beat ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            self.rng = np.random.default_rng()\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self.name = \"Random Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        return self.random_choice_with_mask(np.arange(7), obs_mask[\"action_mask\"])\n",
    "\n",
    "    def random_choice_with_mask(self, arr, mask):\n",
    "        masked_arr = np.ma.masked_array(arr, mask=1 - mask)\n",
    "        if masked_arr.count() == 0:\n",
    "            return None\n",
    "        return self.rng.choice(masked_arr.compressed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayLeftmostLegal:\n",
    "    def __init__(self):\n",
    "        self.name = \"Left Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        for i, legal in enumerate(obs_mask[\"action_mask\"]):\n",
    "            if legal:\n",
    "                return i\n",
    "        return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4fdf7e",
   "metadata": {},
   "source": [
    "# Running a game\n",
    "\n",
    "\n",
    "The following function runs a full game between the two agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent0, agent1, display=False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    while not done:\n",
    "        for i, agent in enumerate([agent0, agent1]):\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            env.step(action)\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            done = terminated\n",
    "            if np.sum(obs[\"action_mask\"]) == 0:\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                return 0.5\n",
    "            if done:\n",
    "                if display:\n",
    "                    print(f\"Player {i}: {agent.name} won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = RandomPlayer()\n",
    "agent1 = PlayLeftmostLegal()\n",
    "\n",
    "play_game(env, agent0, agent1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([play_game(env, agent0, agent1, display=False) for _ in range(100)])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173cc5af",
   "metadata": {},
   "source": [
    "# Emulating a Gym environment\n",
    "\n",
    "If we fix the opposite policy, the game from the point of view of the agent is equivalent to a Gym environment. The following class implements this simulation. Then any algorithm that would work in a gym environment with the same observations will work here. \n",
    "\n",
    "Note that we implemented the possibility to be the first or the second player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvAgainstPolicy: \n",
    "    def __init__(self, env, policy, first_player=True):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.first_player = first_player\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward, terminated, _, _ = self.env.last()\n",
    "        if terminated: \n",
    "            self.last_step = obs, reward, True, False, {}\n",
    "        else: \n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "            obs, reward, terminated, _, _ = self.env.last()\n",
    "            self.last_step = obs, -reward, terminated, False, {}\n",
    "        return self.last_step\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        if not(self.first_player): \n",
    "            obs, _, _, _, _ = self.env.last()\n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "\n",
    "        self.last_step = self.env.last()\n",
    "        return self.last_step\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb3fab1",
   "metadata": {},
   "source": [
    "# Evaluating an agent against a fixed policy: \n",
    "\n",
    "Using the environment above, we can evaluate the agent against this fixed policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_policy(env, agent, policy, N_episodes=10, first_player=True):\n",
    "    eval_env = EnvAgainstPolicy(env, policy, first_player=first_player)\n",
    "    results = []\n",
    "    for _ in range(N_episodes):\n",
    "        done = False\n",
    "        eval_env.reset()\n",
    "        obs, _, _, _, _ = eval_env.last()\n",
    "        while not done:\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            eval_env.step(action)\n",
    "            obs, reward, done, _, _ = eval_env.last()\n",
    "        results.append(reward)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da1790b4",
   "metadata": {},
   "source": [
    "We can see that if both players play randomly, there is a small but significant advantage to the first player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38debdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=False))\n",
    "plt.show()\n",
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=True))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79580594",
   "metadata": {},
   "source": [
    "# Your turn \n",
    "\n",
    "Try to build a decent agent. Be creative! You can try any idea that you have: the grade is not about performance of the agent, but more about illustrating phenomena happening in Reinforcement Learning for turn-based games. It's okay to 'help' the agent in any way, as long as it follows the ideas of RL (i.e., as long as there is some learning involved).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62771e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
